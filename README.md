# Prodigy-Task-1

# Fine-Tuning GPT-2 on Custom Dataset

This repository contains code and instructions for fine-tuning the GPT-2 model on a custom dataset using the Hugging Face Transformers library. The goal is to adapt the pre-trained GPT-2 model for specific text generation tasks, allowing users to generate coherent and contextually relevant text based on their data.

## Table of Contents

- [Features](#features)
- [Requirements](#requirements)
- [Installation](#installation)


## Features

- **Fine-Tuning**: Easily fine-tune the GPT-2 model on your custom text dataset.
- **Text Generation**: Generate coherent and contextually relevant text based on prompts.
- **Evaluation Metrics**: Evaluate model performance using perplexity and other metrics.
- **Customizable Parameters**: Adjust training parameters such as learning rate, batch size, and epochs.

## Requirements

To run this project, you need the following:

- **Python**: Version 3.6 or higher.
- **PyTorch**: Ensure you have the correct version compatible with your CUDA installation (if using GPU).
- **Hugging Face Transformers**: For model and tokenizer handling.
- **Datasets Library**: For loading and processing datasets.

## Installation

Clone this repository and install the required packages
