# Prodigy-Task-1

# Fine-Tuning GPT-2 on Custom Dataset

This repository contains code and instructions for fine-tuning the GPT-2 model on a custom dataset using the Hugging Face Transformers library. The goal is to adapt the pre-trained GPT-2 model for specific text generation tasks.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Dataset Preparation](#dataset-preparation)
- [Fine-Tuning the Model](#fine-tuning-the-model)
- [Generating Text](#generating-text)
- [Evaluation](#evaluation)
- [License](#license)

## Features

- Fine-tune GPT-2 on your custom text dataset.
- Generate coherent and contextually relevant text.
- Evaluate model performance using perplexity.

## Installation

Clone this repository and install the required packages:

```bash
git clone https://github.com/yourusername/gpt2-fine-tuning.git
cd gpt2-fine-tuning
pip install -r requirements.txt